{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random,timeit\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy as dc\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(n,dim):\n",
    "    gens=[[0 if g != j else 1 for g in range(n)] for j in range(dim)]\n",
    "    return gens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(train_d,train_l,gen):\n",
    "        mask=np.array(gen) > 0\n",
    "        al_data=np.array([al[mask] for al in train_d])\n",
    "        kf = ms.KFold(n_splits=4)\n",
    "        s = 0\n",
    "        for tr_ix,te_ix in kf.split(al_data):\n",
    "            s+= svm.LinearSVC().fit(al_data[tr_ix],train_l[tr_ix]).score(al_data[te_ix],train_l[te_ix])#.predict(al_test_data)\n",
    "        s/=4\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bees_optimization(bee,binary,i):\n",
    "    binary=list(binary)\n",
    "    j=random.randint(0,len(binary)-1)\n",
    "    k=random.randint(0,len(binary)-1)\n",
    "    while k==i:\n",
    "        k=random.randint(0,len(binary)-1)\n",
    "    fit=binary[j]+random.uniform(-1,1)*(binary[j]-binary[k])\n",
    "    for x in range(bee):\n",
    "        y=random.randint(0,len(binary)-1)\n",
    "        while y==i:\n",
    "            y=random.randint(0,len(binary)-1)\n",
    "        r=random.uniform(0,1)\n",
    "        if r<=fit:\n",
    "            binary[y]=1\n",
    "        \n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BABCO(train_d,train_l,n=10,max_iter=25,employee_percent=0.5,max_limit=5):\n",
    "    \"\"\"\n",
    "    input:{ Eval_Func: Evaluate_Function, type is class\n",
    "            n: Number of population, default=20\n",
    "            max_iter: Number of max iteration, default=300\n",
    "            }\n",
    "    output:{\n",
    "            Best position: type list(int) [1,0,0,1,.....]\n",
    "            Nunber of 1s in best position: type int [0,1,1,0,1] → 3\n",
    "            }\n",
    "    \"\"\"\n",
    "    employed_bees = int(round(n*employee_percent))\n",
    "    onlooker_bees = n - employed_bees       \n",
    "\n",
    "    dim=len(train_d[0])\n",
    "    global_best=float(\"-inf\")\n",
    "    global_position=tuple([0]*dim)\n",
    "    gens_dict = {}\n",
    "    limit=[0]*dim\n",
    "    gens=random_search(dim,dim)\n",
    "    for gen in gens:\n",
    "        if tuple(gen) in gens_dict:\n",
    "            score = gens_dict[tuple(gen)]\n",
    "        else:\n",
    "            score=evaluate(train_d,train_l,gen)\n",
    "            gens_dict[tuple(gen)]=score\n",
    "        if score > global_best:\n",
    "            global_best=score\n",
    "            global_position=dc(gen)\n",
    "            \n",
    "            \n",
    "    for it in range(max_iter):\n",
    "        for i in range(employed_bees):\n",
    "            for i,x in enumerate(gens):\n",
    "                gen=bees_optimization(employed_bees,x,i)\n",
    "                if tuple(gen) in gens_dict:\n",
    "                    score = gens_dict[tuple(gen)]\n",
    "                else:\n",
    "                    score=evaluate(train_d,train_l,gen)\n",
    "                    gens_dict[tuple(gen)]=score\n",
    "\n",
    "                if score > gens_dict[tuple(gens[i])]:\n",
    "                    limit[i]=0\n",
    "                    gens[i]= gen\n",
    "                else:\n",
    "                    limit[i]+=1\n",
    "\n",
    "                if score > global_best:\n",
    "                    global_best=score\n",
    "                    global_position=dc(gen)\n",
    "\n",
    "                if limit[i]>=max_limit:\n",
    "                    gens[i]=[0 if g != i else 1 for g in range(dim)]\n",
    "    \n",
    "        for i in range(onlooker_bees):\n",
    "            for i,x in enumerate(gens):\n",
    "                gen=bees_optimization(employed_bees,x,i)\n",
    "                if tuple(gen) in gens_dict:\n",
    "                    score = gens_dict[tuple(gen)]\n",
    "                else:\n",
    "                    score=evaluate(train_d,train_l,gen)\n",
    "                    gens_dict[tuple(gen)]=score\n",
    "\n",
    "                if score > gens_dict[tuple(gens[i])]:\n",
    "                    limit[i]=0\n",
    "                    gens[i]= gen\n",
    "                else:\n",
    "                    limit[i]+=1\n",
    "\n",
    "                if score > global_best:\n",
    "                    global_best=score\n",
    "                    global_position=dc(gen)\n",
    "\n",
    "                if limit[i]>=max_limit:\n",
    "                    gens[i]=[0 if g != i else 1 for g in range(dim)]\n",
    "\n",
    "                \n",
    "    return global_position,global_position.count(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "def test_score(gen, tr_x, tr_y, te_x, te_y):\n",
    "    mask = np.array(gen) == 1\n",
    "    al_data = np.array(tr_x[:, mask])\n",
    "    al_test_data = np.array(te_x[:, mask])\n",
    "\n",
    "    accuracies, precisions, recalls, f1s = [], [], [], []\n",
    "    for _ in range(5):\n",
    "        model = SVC(kernel='rbf', C=1.0, gamma='scale') \n",
    "        model.fit(al_data, tr_y)\n",
    "        preds = model.predict(al_test_data)\n",
    "\n",
    "        accuracies.append(accuracy_score(te_y, preds))\n",
    "        precisions.append(precision_score(te_y, preds, zero_division=0))\n",
    "        recalls.append(recall_score(te_y, preds, zero_division=0))\n",
    "        f1s.append(f1_score(te_y, preds, zero_division=0))\n",
    "\n",
    "    return (\n",
    "        np.mean(accuracies),\n",
    "        np.mean(precisions),\n",
    "        np.mean(recalls),\n",
    "        np.mean(f1s)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (569, 30)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"/Users/ximenacanton/Documents/TEC/5 SEM/bio inspirado/reto/Bio-inspired-algorithms-for-diagnosis-of-breast-cancer/wdbc.data.csv\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "# id (columna 0), diagnosis (columna 1), resto son features numéricas\n",
    "x = np.array([[float(d) for d in line.split(',')[2:]] for line in lines])  # desde la 3ra columna\n",
    "y = np.array([line.split(',')[1] for line in lines])  # diagnosis ('M' o 'B')\n",
    "\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "training_scores_encoded = lab_enc.fit_transform(y)\n",
    "\n",
    "train_d, test_d, train_l, test_l = train_test_split(x, training_scores_encoded, test_size=0.25)\n",
    "\n",
    "\n",
    "print(\"Shape X:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features → Acc:0.9091  Prec:0.9556  Rec:0.7963  F1:0.8687\n",
      "01  111101000011011110000100000010  13  Acc:0.8741  F1:0.8125\n",
      "02  111111111111010011111010110110  23  Acc:0.8741  F1:0.8125\n",
      "03  111110010010100000101000000010  11  Acc:0.8741  F1:0.8125\n",
      "04  111100001100001000011000101011  13  Acc:0.8741  F1:0.8125\n",
      "05  111101111100101111111010011011  22  Acc:0.8671  F1:0.8000\n",
      "06  110010010000000010011010010111  12  Acc:0.9161  F1:0.8824\n",
      "07  111111010010000001011000100010  13  Acc:0.8741  F1:0.8125\n",
      "08  000011100001100111011110110110  16  Acc:0.9371  F1:0.9109\n",
      "09  000000000001010000100101010010  7  Acc:0.9021  F1:0.8600\n",
      "10  010111100011111100110111111110  21  Acc:0.9091  F1:0.8687\n",
      "11  001101100100000000001011010011  11  Acc:0.9091  F1:0.8687\n",
      "12  010011101110101100011001100111  17  Acc:0.9021  F1:0.8600\n",
      "13  010000011101010000100101011010  12  Acc:0.9021  F1:0.8600\n",
      "14  011010100000001100001000000111  10  Acc:0.8951  F1:0.8454\n",
      "15  111100110000001011111010001000  14  Acc:0.8671  F1:0.8000\n",
      "16  100100111111110101100100110110  18  Acc:0.8741  F1:0.8125\n",
      "17  110100100110001111111010111100  18  Acc:0.8671  F1:0.8000\n",
      "18  110110101000101100001010000110  13  Acc:0.8671  F1:0.8000\n",
      "19  011000110100001011101110010101  15  Acc:0.9371  F1:0.9091\n",
      "20  111110010100101001111101110111  20  Acc:0.9091  F1:0.8687\n",
      "\n",
      "Final: 111110100100001000111010010110\n",
      "Selected features: 14.95\n",
      "Average metrics → Acc:0.8916  Prec:0.9410  Rec:0.7602  F1:0.8404\n",
      "Execution time: 104.0242 s\n"
     ]
    }
   ],
   "source": [
    "# Evaluación inicial con todas las características\n",
    "k = [1 for r in range(len(x[0]))]\n",
    "acc, prec, rec, f1 = test_score(k, train_d, train_l, test_d, test_l)\n",
    "print(f\"All features → Acc:{acc:.4f}  Prec:{prec:.4f}  Rec:{rec:.4f}  F1:{f1:.4f}\")\n",
    "\n",
    "# Inicialización de métricas\n",
    "fattr = 0\n",
    "ftest_acc = 0.0\n",
    "ftest_prec = 0.0\n",
    "ftest_rec = 0.0\n",
    "ftest_f1 = 0.0\n",
    "\n",
    "flist = [0 for _ in range(len(x[0]))]\n",
    "final_list = [0 for _ in range(len(x[0]))]\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Repeticiones\n",
    "for i in range(20):\n",
    "    g, l = BABCO(train_d, train_l, n=10, max_iter=25, employee_percent=0.5, max_limit=5)\n",
    "    fattr += l\n",
    "\n",
    "    acc, prec, rec, f1 = test_score(g, train_d, train_l, test_d, test_l)\n",
    "    ftest_acc += acc\n",
    "    ftest_prec += prec\n",
    "    ftest_rec += rec\n",
    "    ftest_f1 += f1\n",
    "\n",
    "    for j in range(len(flist)):\n",
    "        if g[j] == 1:\n",
    "            flist[j] += 1\n",
    "\n",
    "    print(f\"{i+1:02d}  {''.join(map(str,g))}  {l}  Acc:{acc:.4f}  F1:{f1:.4f}\")\n",
    "\n",
    "# Promedios finales\n",
    "fattr /= 20\n",
    "ftest_acc /= 20\n",
    "ftest_prec /= 20\n",
    "ftest_rec /= 20\n",
    "ftest_f1 /= 20\n",
    "\n",
    "end = timeit.default_timer()\n",
    "time = end - start\n",
    "\n",
    "# Selección final de características\n",
    "final_count = int(round(fattr))\n",
    "final_count = max(0, min(final_count, len(flist)))\n",
    "final = np.argsort(flist)[::-1][:final_count]\n",
    "for i in range(len(final)):\n",
    "    final_list[final[i]] = 1\n",
    "\n",
    "print(f\"\\nFinal: {''.join(map(str,final_list))}\")\n",
    "print(f\"Selected features: {fattr:.2f}\")\n",
    "print(f\"Average metrics → Acc:{ftest_acc:.4f}  Prec:{ftest_prec:.4f}  Rec:{ftest_rec:.4f}  F1:{ftest_f1:.4f}\")\n",
    "print(f\"Execution time: {time:.4f} s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random,math,copy,timeit,operator\n",
    "from sklearn import utils,preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy as dc\n",
    "from sys import exit\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection as ms"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.random.seed(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ants(object):\n",
    "    def __init__(self,gens=None,mse=None):\n",
    "        self.gens=gens\n",
    "        self.mse=mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(n,dim):\n",
    "    gens=[[0 for g in range(dim)] for _ in range(n)]\n",
    "    for i,gen in enumerate(gens) :\n",
    "        r=random.randint(1,dim)\n",
    "        for _r in range(r):\n",
    "            gen[_r]=1\n",
    "        random.shuffle(gen)\n",
    "    return gens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_search(n,dim,flist,pheromone):\n",
    "    gens=[[0 for g in range(dim)] for _ in range(n)]\n",
    "    for i,gen in enumerate(gens) :\n",
    "        r=random.randint(1,dim)\n",
    "        for _r in range(r):\n",
    "            gen[_r]=1\n",
    "        random.shuffle(gen)\n",
    "        for j,x in enumerate(gen):\n",
    "            gen[j]=flist[j]*gen[j]\n",
    "    return gens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(train_d,train_l,gen):\n",
    "        mask=np.array(gen) > 0\n",
    "        al_data=np.array([al[mask] for al in train_d])\n",
    "        kf = ms.StratifiedKFold(n_splits=4)\n",
    "        s = 0\n",
    "        for tr_ix,te_ix in kf.split(al_data, train_l):\n",
    "            s+= svm.LinearSVC().fit(al_data[tr_ix],train_l[tr_ix]).score(al_data[te_ix],train_l[te_ix])#.predict(al_test_data)\n",
    "        s/=4\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef evaluate(train_d,train_l,gen):\\n        mask = np.array(gen) > 0\\n        # if no features selected, return worst/zero score to avoid classifier errors\\n        if np.sum(mask) == 0:\\n            return 0.0\\n        al_data = np.array([al[mask] for al in train_d])\\n        kf = ms.StratifiedKFold(n_splits=4)\\n        s = 0\\n        for tr_ix,te_ix in kf.split(al_data, train_l):\\n            try:\\n                s += svm.LinearSVC(max_iter=5000).fit(al_data[tr_ix],train_l[tr_ix]).score(al_data[te_ix],train_l[te_ix])\\n            except Exception:\\n                # if classifier fails for any reason, treat as zero performance\\n                return 0.0\\n        s /= 4\\n        return s\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(train_d,train_l,gen):\n",
    "        mask = np.array(gen) > 0\n",
    "        # if no features selected, return worst/zero score to avoid classifier errors\n",
    "        if np.sum(mask) == 0:\n",
    "            return 0.0\n",
    "        al_data = np.array([al[mask] for al in train_d])\n",
    "        kf = ms.StratifiedKFold(n_splits=4)\n",
    "        s = 0\n",
    "        for tr_ix,te_ix in kf.split(al_data, train_l):\n",
    "            try:\n",
    "                s += svm.LinearSVC(max_iter=5000).fit(al_data[tr_ix],train_l[tr_ix]).score(al_data[te_ix],train_l[te_ix])\n",
    "            except Exception:\n",
    "                # if classifier fails for any reason, treat as zero performance\n",
    "                return 0.0\n",
    "        s /= 4\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BACO(train_d,train_l,n=30,max_iter=25,T0=0.1,k=10,m=25,p=3,rho=0.25):\n",
    "    \"\"\"\n",
    "    input:{ Eval_Func: Evaluate_Function, type is class\n",
    "            n: Number of population, default=20\n",
    "            max_iter: Number of max iteration, default=300\n",
    "            }\n",
    "    output:{\n",
    "            Best position: type list(int) [1,0,0,1,.....]\n",
    "            Nunber of 1s in best position: type int [0,1,1,0,1] → 3\n",
    "            }\n",
    "    \"\"\"\n",
    "    dim=len(train_d[0])\n",
    "    global_best=float(\"-inf\")\n",
    "    pheromone=list([T0]*dim)\n",
    "    pher_diff=list([0]*dim)\n",
    "    global_position=tuple([0]*dim)\n",
    "    antslist=[]\n",
    "    gens_dict = {}\n",
    "    for i in range(n):\n",
    "        gens=random_search(m,dim)\n",
    "        mse=0\n",
    "        for gen in gens:\n",
    "            if tuple(gen) in gens_dict:\n",
    "                score = gens_dict[tuple(gen)]\n",
    "            else:\n",
    "                score=evaluate(train_d,train_l,gen)\n",
    "                gens_dict[tuple(gen)]=score\n",
    "            global_best=score\n",
    "            global_position=dc(gen)\n",
    "            mse+=score\n",
    "        mse=mse/m\n",
    "        antslist.append(ants(gens,mse))\n",
    "    antslist.sort(key=operator.attrgetter('mse'), reverse=True)\n",
    "    klist=list(antslist[i] for i in range(k))\n",
    "    mselist=list(antslist[i].mse for i in range(k))\n",
    "    for j in range(len(klist)):\n",
    "        diff = (max(mselist) - mselist[j]) / (max(mselist) - min(mselist) + 1e-9)\n",
    "        flist=list([0]*dim)\n",
    "        for gen in klist[j].gens:\n",
    "            for a,x in enumerate(gen):\n",
    "                if x==1:\n",
    "                    pher_diff[a]=diff\n",
    "                    flist[a]=1\n",
    "                else:\n",
    "                    pher_diff[a]=0\n",
    "    \n",
    "    for i in range(len(pheromone)):\n",
    "        pheromone[i]=pheromone[i]*rho + pher_diff[i]\n",
    "        \n",
    "        \n",
    "    for it in range(max_iter):\n",
    "        m=m-p\n",
    "        for i in range(n):\n",
    "            gens=k_search(m,dim,flist,pheromone)\n",
    "            mse=0\n",
    "            for gen in gens:\n",
    "                if tuple(gen) in gens_dict:\n",
    "                    score = gens_dict[tuple(gen)]\n",
    "                else:\n",
    "                    score=evaluate(train_d,train_l,gen)\n",
    "                    gens_dict[tuple(gen)]=score\n",
    "                global_best=score\n",
    "                global_position=dc(gen)\n",
    "                mse+=score\n",
    "            mse=mse/m\n",
    "            antslist.append(ants(gens,mse))\n",
    "        antslist.sort(key=operator.attrgetter('mse'), reverse=True)\n",
    "        klist=list(antslist[i] for i in range(k))\n",
    "        mselist=list(antslist[i].mse for i in range(k))\n",
    "        for j in range(len(klist)):\n",
    "            diff = (max(mselist) - mselist[j]) / (max(mselist) - min(mselist) + 1e-9)\n",
    "            flist=list([0]*dim)\n",
    "            for gen in klist[j].gens:\n",
    "                for i,x in enumerate(gen):\n",
    "                    if x==1:\n",
    "                        pher_diff[i]=diff\n",
    "                        flist[i]=1\n",
    "                    else:\n",
    "                        pher_diff[i]=0\n",
    "    \n",
    "        for i in range(len(pheromone)):\n",
    "            pheromone[i]=pheromone[i]*rho + pher_diff[i]\n",
    "\n",
    "        \n",
    "    return global_position,global_position.count(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "def test_score(gen, tr_x, tr_y, te_x, te_y):\n",
    "    mask = np.array(gen) == 1\n",
    "    al_data = np.array(tr_x[:, mask])\n",
    "    al_test_data = np.array(te_x[:, mask])\n",
    "\n",
    "    accuracies, precisions, recalls, f1s = [], [], [], []\n",
    "    for _ in range(5):\n",
    "        model = SVC(kernel='rbf', C=1.0, gamma='scale') \n",
    "        model.fit(al_data, tr_y)\n",
    "        preds = model.predict(al_test_data)\n",
    "\n",
    "        accuracies.append(accuracy_score(te_y, preds))\n",
    "        precisions.append(precision_score(te_y, preds, zero_division=0))\n",
    "        recalls.append(recall_score(te_y, preds, zero_division=0))\n",
    "        f1s.append(f1_score(te_y, preds, zero_division=0))\n",
    "\n",
    "    return (\n",
    "        np.mean(accuracies),\n",
    "        np.mean(precisions),\n",
    "        np.mean(recalls),\n",
    "        np.mean(f1s)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (569, 30)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"../../data/wdbc.data.csv\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "# id (columna 0), diagnosis (columna 1), resto son features numéricas\n",
    "x = np.array([[float(d) for d in line.split(',')[2:]] for line in lines])  # desde la 3ra columna\n",
    "y = np.array([line.split(',')[1] for line in lines])  # diagnosis ('M' o 'B')\n",
    "\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "training_scores_encoded = lab_enc.fit_transform(y)\n",
    "\n",
    "train_d, test_d, train_l, test_l = train_test_split(x, training_scores_encoded, test_size=0.25)\n",
    "\n",
    "\n",
    "print(\"Shape X:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float64(0.944055944055944), np.float64(0.9811320754716981), np.float64(0.8813559322033898), np.float64(0.9285714285714286))\n",
      " 1  111111111111111111111111111111  30.0000  Acc:0.9441  F1:0.9286\n",
      " 2  100100010000001000000000011000  6.0000  Acc:0.8951  F1:0.8598\n",
      " 3  100111001111101011111011010111  21.0000  Acc:0.9441  F1:0.9286\n",
      " 4  111111101011111111111111111111  28.0000  Acc:0.9441  F1:0.9286\n",
      " 5  000000000001011000010001010110  8.0000  Acc:0.9441  F1:0.9298\n",
      " 6  111111100100110011110110100110  19.0000  Acc:0.9091  F1:0.8807\n",
      " 7  001000111110011100010010101000  13.0000  Acc:0.9580  F1:0.9464\n",
      " 8  100011110111101111011111011111  23.0000  Acc:0.9441  F1:0.9298\n",
      " 9  011001111111111110010011100110  20.0000  Acc:0.9441  F1:0.9298\n",
      "10  111111110111111011111111111111  28.0000  Acc:0.9441  F1:0.9286\n",
      "11  100001111101110011100101101100  17.0000  Acc:0.9441  F1:0.9298\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "k = [1 for r in range(len(x[0]))]\n",
    "print(test_score(k, train_d, train_l, test_d, test_l))\n",
    "\n",
    "fattr = 0\n",
    "# per-metric accumulators\n",
    "ftest_acc = 0.0\n",
    "ftest_prec = 0.0\n",
    "ftest_rec = 0.0\n",
    "ftest_f1 = 0.0\n",
    "\n",
    "flist = [0 for _ in range(len(x[0]))]\n",
    "final_list = [0 for _ in range(len(x[0]))]\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for i in range(20):\n",
    "    g, l = BACO(train_d, train_l, n=30, max_iter=25, T0=0.1, k=10, m=25, p=3, rho=0.25)\n",
    "    \n",
    "    # Asegurar que 'l' sea numérico\n",
    "    if isinstance(l, list):\n",
    "        l = len(l)\n",
    "    elif not isinstance(l, (int, float)):\n",
    "        l = float(l)\n",
    "\n",
    "    fattr += l\n",
    "\n",
    "    # unpack metrics\n",
    "    acc, prec, rec, f1 = test_score(g, train_d, train_l, test_d, test_l)\n",
    "    ftest_acc += acc\n",
    "    ftest_prec += prec\n",
    "    ftest_rec += rec\n",
    "    ftest_f1 += f1\n",
    "    \n",
    "    for j in range(len(flist)):\n",
    "        if g[j] == 1:\n",
    "            flist[j] += 1\n",
    "\n",
    "    # print per-iteration metrics (Acc and F1 shown)\n",
    "    print(f\"{i + 1:2d}  {''.join(map(str, g))}  {l:.4f}  Acc:{acc:.4f}  F1:{f1:.4f}\")\n",
    "\n",
    "# Promedios\n",
    "fattr /= 20\n",
    "ftest_acc /= 20\n",
    "ftest_prec /= 20\n",
    "ftest_rec /= 20\n",
    "ftest_f1 /= 20\n",
    "\n",
    "end = timeit.default_timer()\n",
    "time = end - start\n",
    "\n",
    "# Asegurar que fattr esté dentro de rango y sea entero\n",
    "final_count = int(round(fattr))\n",
    "final_count = max(0, min(final_count, len(flist)))\n",
    "\n",
    "final = np.argsort(flist)[::-1][:final_count]\n",
    "\n",
    "for idx in final:\n",
    "    final_list[idx] = 1\n",
    "\n",
    "print(f\"Final:  {''.join(map(str, final_list))}   {fattr:.4f}   Acc:{ftest_acc:.6f}  F1:{ftest_f1:.6f}    {time:.4f}\")\n",
    "# ...existing code..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mv-tec)",
   "language": "python",
   "name": "mv-tec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
